{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9062623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topk = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b492b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/datasets/kenneth_lay/enron_roles.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-12e3981cabc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mroles_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/datasets/kenneth_lay/enron_roles.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mroles_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mroles_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroles_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mroles_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'emailid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroles_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\naheedanjum.arafat\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/datasets/kenneth_lay/enron_roles.txt'"
     ]
    }
   ],
   "source": [
    "roles_path = '../data/datasets/kenneth_lay/enron_roles.txt'\n",
    "roles_dict = {}\n",
    "roles_df = pd.read_csv(roles_path,sep=\",\")\n",
    "roles_df.columns = ['emailid','name','position']\n",
    "print(roles_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea2ef3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgname = '../data/datasets/real/klay_s.hyp'\n",
    "# hgname='../data/datasets/real/meetup.hyp'\n",
    "hyperedges = []\n",
    "vertex_set = set()\n",
    "with open(hgname,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        edge = line.split(',')\n",
    "        edge = [e.strip() for e in edge]\n",
    "        hyperedges.append(edge)\n",
    "        for v in edge:\n",
    "            vertex_set.add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68492ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                     Message-ID  \\\n",
      "0          0  <19937858.1075858631491.JavaMail.evans@thyme>   \n",
      "1          1  <20682410.1075859177338.JavaMail.evans@thyme>   \n",
      "2          2  <25469111.1075840337794.JavaMail.evans@thyme>   \n",
      "3          4  <31732110.1075849814292.JavaMail.evans@thyme>   \n",
      "4          5   <5072789.1075855783561.JavaMail.evans@thyme>   \n",
      "\n",
      "                        Date  \\\n",
      "0  2001-10-18 11:52:23-07:00   \n",
      "1  2001-07-14 12:33:04-07:00   \n",
      "2  2001-08-23 11:24:25-07:00   \n",
      "3  2001-01-03 10:58:00-08:00   \n",
      "4  2000-08-25 08:26:00-07:00   \n",
      "\n",
      "                                             Subject  \\\n",
      "0  Quarterly Managing Director Meeting - Monday, ...   \n",
      "1  FW: Iris Mack's involvement in the Enron Team ...   \n",
      "2                          Associate/Analyst Program   \n",
      "3                             1/2/01 Preliminary DPR   \n",
      "4                                      DPR - 8/24/00   \n",
      "\n",
      "                                            X-Folder    X-Origin  \\\n",
      "0  \\PALLEN (Non-Privileged)\\Allen, Phillip K.\\Del...     Allen-P   \n",
      "1   \\Harry_Arora_Jan2002\\Arora, Harry\\Inbox\\Receipts     Arora-H   \n",
      "2   \\ExMerge - Baughman Jr., Don\\Enron Power\\24 Hour  BAUGHMAN-D   \n",
      "3    \\Sally_Beck_Nov2001\\Notes Folders\\All documents      BECK-S   \n",
      "4    \\Sally_Beck_Dec2000\\Notes Folders\\All documents      Beck-S   \n",
      "\n",
      "                    X-FileName  \\\n",
      "0  PALLEN (Non-Privileged).pst   \n",
      "1  harora (Non-Privileged).pst   \n",
      "2     don baughman 6-25-02.PST   \n",
      "3                    sbeck.nsf   \n",
      "4                    sbeck.nsf   \n",
      "\n",
      "                                             content        user  \\\n",
      "0  Please plan to attend the Quarterly Managing D...     allen-p   \n",
      "1  \\nDear Dr. Lay,\\n\\n\\tMy name is Iris Mack.  My...     arora-h   \n",
      "2   \\n\\nTo:\\tAssociate/Analyst Program Worldwide\\...  baughman-d   \n",
      "3     \\nA preliminary Daily Position Report has b...      beck-s   \n",
      "4  The DPR for today is final and posted to the E...      beck-s   \n",
      "\n",
      "                                        participants  ... Unnamed: 34  \\\n",
      "0  jim.prentice@enron.com,mark.frevert@enron.com,...  ...         NaN   \n",
      "1          iris.mack@enron.com,kenneth.lay@enron.com  ...         NaN   \n",
      "2  lindsay.culotta@enron.com,mark.frevert@enron.c...  ...         NaN   \n",
      "3  mark.haedicke@enron.com,jeffrey.shankman@enron...  ...         NaN   \n",
      "4  mark.haedicke@enron.com,jeffrey.shankman@enron...  ...         NaN   \n",
      "\n",
      "  Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 41 Unnamed: 42 Unnamed: 43  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "1190\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('../data/datasets/kenneth_lay/klay_s.csv',sep=',',engine = 'python')\n",
    "df = pd.read_csv('../../Hypergraph codes/Cpp-Coredecomp/data/datasets/kenneth_lay/klay_s.csv',sep=',',engine='python')\n",
    "print(df.head())\n",
    "print(len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "987cee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vertex:  4718  #edge:  1190\n"
     ]
    }
   ],
   "source": [
    "print('#vertex: ',len(vertex_set),' #edge: ',len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbe28246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(a, fname = 'tmp.pickle'):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_dict(fname = 'tmp.pickle'):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "        return b\n",
    "    \n",
    "def retrieve_email(integer_id,mapN='../data/datasets/kenneth_lay/klay_idtoemail.map'):\n",
    "    _map = load_dict(mapN)\n",
    "    if str(integer_id) not in _map:\n",
    "        return None\n",
    "    return _map[str(integer_id)]\n",
    "\n",
    "def retrieve_list_emails(lst_integer_id,mapN='../data/datasets/kenneth_lay/klay_idtoemail.map'):\n",
    "    _map = load_dict(mapN)\n",
    "    # print(_map)\n",
    "    _tmp = []\n",
    "    for integer_id in lst_integer_id:\n",
    "        if str(integer_id) not in _map:\n",
    "            return None\n",
    "        else:\n",
    "            _tmp.append(_map[str(integer_id)])\n",
    "            # print(_tmp)\n",
    "    return _tmp\n",
    "\n",
    "def retrieve_stronglyinduced_emails(lst_integer_id, edgeList, df):\n",
    "    subhyp = []\n",
    "    for i,value in enumerate(edgeList):\n",
    "        flag = True\n",
    "        for u in value:\n",
    "            if u not in lst_integer_id:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            subhyp.append(i)\n",
    "        \n",
    "    return subhyp, df.filter(items = subhyp, axis=0)\n",
    "\n",
    "# def retrieve_weeklyinduced_complexes(lst_integer_id,human_hg):\n",
    "#     protein_complex_ids = []\n",
    "#     for key,value in human_hg.items():\n",
    "#         flag = True\n",
    "#         for u in value:\n",
    "#             if u in lst_integer_id:\n",
    "#                 protein_complex_ids.append(key)\n",
    "#                 break \n",
    "#     return protein_complex_ids\n",
    "def load_hg_ascsv(name):\n",
    "    _list = []\n",
    "    with open(name,'r') as f:\n",
    "        for line in f:\n",
    "            e = [i for i in line.strip().split(',')]\n",
    "            _list.append(e)\n",
    "    return _list\n",
    "\n",
    "def query_position(email):\n",
    "    em_id = email.split('@')[0]\n",
    "    _tmp = roles_df[roles_df.emailid == em_id]\n",
    "    if len(_tmp) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return roles_df[roles_df.emailid == em_id].position.values[0]\n",
    "def query_name(email):\n",
    "    em_id = email.split('@')[0]\n",
    "    _tmp = roles_df[roles_df.emailid == em_id]\n",
    "    if len(_tmp) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return roles_df[roles_df.emailid == em_id].name.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a9972",
   "metadata": {},
   "source": [
    "Densest Subgraph Analysis\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4340cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166   1949   1949\n",
      "202   122   907396\n",
      "degree-Densest sub-protein complexes: \n",
      "6      EnronOnline Executive Summary for April 20, 2001\n",
      "7     EnronOnline Executive Summary for December 12,...\n",
      "8        EnronOnline Executive Summary for May 31, 2001\n",
      "12                     NEW POLL FOR MC EXTENDED MEETING\n",
      "13                 New Poll for the MC Extended Meeting\n",
      "Name: Subject, dtype: object\n",
      "volume-Densest sub-protein complexes: \n",
      "2                        Associate/Analyst Program\n",
      "19                     Center for Houston's Future\n",
      "24    Project Southwood Meeting  - Location Change\n",
      "95                                          UC/CSU\n",
      "97                      III Summit of the Americas\n",
      "Name: Subject, dtype: object\n",
      "122\n",
      "clique-Densest sub-protein complexes: \n",
      "2                        Associate/Analyst Program\n",
      "19                     Center for Houston's Future\n",
      "24    Project Southwood Meeting  - Location Change\n",
      "95                                          UC/CSU\n",
      "97                      III Summit of the Americas\n",
      "Name: Subject, dtype: object\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "dataset = 'klay'\n",
    "\n",
    "deg_d = '../output/density/'+dataset+'_deg.csv'\n",
    "vol_d = '../output/density/'+dataset+'_nbr.csv'\n",
    "ck_d = '../output/density/'+dataset+'_clique.csv'\n",
    "\n",
    "deg_dhg = load_hg_ascsv(deg_d)\n",
    "vol_dhg = load_hg_ascsv(vol_d)\n",
    "ck_dhg = load_hg_ascsv(ck_d)\n",
    "nodes_degD = set()\n",
    "for e in deg_dhg:\n",
    "    for u in e:\n",
    "        nodes_degD.add(u)\n",
    "\n",
    "nodes_volD = set()\n",
    "for e in vol_dhg:\n",
    "    for u in e:\n",
    "        nodes_volD.add(u)\n",
    "\n",
    "nodes_ckD = set()\n",
    "for e in ck_dhg:\n",
    "    for u in e:\n",
    "        nodes_ckD.add(u)\n",
    "        \n",
    "print(len(nodes_degD),' ',len(nodes_volD),' ',len(nodes_ckD))\n",
    "print(len(deg_dhg),' ',len(vol_dhg),' ',len(ck_dhg))\n",
    "# print(nodes_degD)\n",
    "coreprotein_names = retrieve_list_emails(nodes_degD,mapN='../data/datasets/kenneth_lay/'+dataset+'_idtoemail.map')\n",
    "# print(coreprotein_names)\n",
    "sub_complexes_degD,deg_df = retrieve_stronglyinduced_emails(nodes_degD, hyperedges,df)\n",
    "print('degree-Densest sub-protein complexes: ')\n",
    "print(deg_df['Subject'].head())\n",
    "coreprotein_names = retrieve_list_emails(nodes_volD,mapN='../data/datasets/kenneth_lay/'+dataset+'_idtoemail.map')\n",
    "# print(coreprotein_names)\n",
    "sub_complexes_volD,vol_df = retrieve_stronglyinduced_emails(nodes_volD, hyperedges,df)\n",
    "print('volume-Densest sub-protein complexes: ')\n",
    "print(vol_df['Subject'].head())\n",
    "print(len(sub_complexes_volD))\n",
    "coreprotein_names = retrieve_list_emails(nodes_ckD,mapN='../data/datasets/kenneth_lay/'+dataset+'_idtoemail.map')\n",
    "# print(coreprotein_names)\n",
    "sub_complexes_ckD,ck_df = retrieve_stronglyinduced_emails(nodes_ckD, hyperedges,df)\n",
    "print('clique-Densest sub-protein complexes: ')\n",
    "print(ck_df['Subject'].head())\n",
    "print(len(sub_complexes_ckD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddaa8b4-219c-4ceb-b6a7-fd83d6d15bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-Densest sub-hyp Events: \n",
      "['Trivia Night at Plantation Pub', 'Trivia Night at Plantation Pub', \"1st Tuesday Ladies Lunch at Chili's Bellevue\", 'Wednesday night dinner at Voodoo Gumbo', 'Tuesday night dinner at {Pub}licity', 'Dinner At KYOTO', \"Dinner At Dalton's Grill\", 'Dinner At Plantation Pub Formerly Three Stones', 'Dinner at Lucky Bamboo China Bistro', \"Beginner's Hike at Edwin Warner, Sun. 6:30\", 'Dinner at Lemongrass Thai and Sushi', \"Trivia Night at Nacho's Mexican Restaurant HWY 100\", \"Trivia Night at Nacho's Mexican Restaurant HWY 100\", \"Trivia Night at Nacho's Mexican Restaurant HWY 100\", 'Ladies Lunch Thursday June 9', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Trivia Night at Three Stones Pub', 'Beginners Hike at Edwin Warner Jan. 3', \"Celebrate New Year's Eve with Friends\", 'Last-Minute Dinner Tonight at Corner Pub 6:30', \"HO! HO! HO! Let's spread some Christmas cheer with the Santa Rampage downtown.\"]\n",
      "average #participants:  4.423076923076923\n",
      "vol. density:  9.272727272727273\n",
      "volume-Densest sub-hyp Events: \n",
      "['The Regulatory Environment Around Blockchain', 'Nashville Blockchain Meetup-  Ethereum- The World Computer', 'Bitcoin / Cryptocurrency Basics (An Introduction)', 'Web Scraping in Python', 'An overview of Google Cloud Datalab', 'Nashville PHP South Lunch', '2016 Nashville Filmmakers Premiere', 'ActiveRecord 2: Electric Boogaloo', 'Identity and Access Controls Landscape in .NET', 'Cyber Terrorism & Security', 'IT Industry & Career Panel', 'Learning Xamarin, Where to Start with Denise Tinsley', 'From Developer to Data Scientist with Gaines Kergosien', 'IT Industry & Career Panel', 'Intro to AngularJS for ASP.NET Developers with Matt Honeycutt', 'A look inside the ASP.NET MVC Membership Provider with Michael McCann', 'How to Build Beautiful ASP.NET MVC Apps for Non-Designers with Matt Honeycutt', 'PyNash x Penny U: An Evening of Learning', 'datetime in Python: What Time is it Anyway?', 'Get a grip w/ the GamePad API for web browsers / Matthew McCord', 'Live Coding - NashJobs API + Slack Bot \\x97 React/Node', 'Identity and Access Control Landscape - Travis Shepherd', \"Web Assembly, What's the big deal? with Courey Elliott\", 'End to End Testing in JavaScript with Selenium and WebdriverIO / Josh Cypher', 'Progressive Web Apps', 'Monthly Transplant Meetup', 'W.O.M.E.N. Come Celebrate our HUGE Milestone!']\n",
      "average #participants:  77.81481481481481\n",
      "volume-density:  116.8926123381569\n",
      "clique-Densest sub-hyp Events: \n",
      "['Field Trip w/ Genealogical Society', 'Field Trip w/ Genealogical Society', 'Field Trip with Mid-Tenn Genealogical Society to TSLA', 'Lisp Sync', 'Lisp Sync', 'Lisp Sync', 'Lisp Sync', 'Lisp Sync', 'Lisp Sync', \"We're Invited: Nashville  Day of Civic Hacking 2017\", 'Monthly Meeting \"Civic Tech\"', 'Barcamp Nashville', 'Nashville PHP South Lunch', 'Nashville Mini Maker Faire', 'Testing a React App', 'Introduction to SQL Part Two', 'ActiveRecord 2: Electric Boogaloo']\n",
      "average #participants:  4.588235294117647\n",
      "volume-density:  0.3963963963963964  degree-density as clique:  100.68243243243244\n",
      "|V*_deg|= 11  |V*_vol|= 1313  |V*_clique|= 888\n",
      "|E*_deg|= 26  |E*_vol|= 27  |E*_clique|= 17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "M = '../data/datasets/real/meetup_eidtoevent.map'\n",
    "dataset = 'meetup'\n",
    "\n",
    "deg_d = '../output/density/'+dataset+'_deg.csv'\n",
    "vol_d = '../output/density/'+dataset+'_nbr.csv'\n",
    "ck_d = '../output/density/'+dataset+'_clique.csv'\n",
    "\n",
    "efile_degD = '../output/density/'+dataset+'_deg_e.csv'\n",
    "efile_volD = '../output/density/'+dataset+'_nbr_e.csv'\n",
    "efile_ckD = '../output/density/'+dataset+'_clique_e.csv'\n",
    "\n",
    "def load_dict(fname = 'tmp.pickle'):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "        return b\n",
    "\n",
    "def load_hg_ascsv(name):\n",
    "    _list = []\n",
    "    with open(name,'r') as f:\n",
    "        for line in f:\n",
    "            e = [i for i in line.strip().split(',')]\n",
    "            _list.append(e)\n",
    "    return _list\n",
    "\n",
    "def retrieve_list_events(densest_subeFile, mapN='../data/datasets/real/meetup_eidtoevent.map'):\n",
    "    _map = load_dict(mapN)\n",
    "    densest_subedges = []\n",
    "    with open(densest_subeFile,'r') as f:\n",
    "        for line in f:\n",
    "            densest_subedges.append(int(line.strip()))\n",
    "    # print(_map)\n",
    "    _tmp = []\n",
    "    for integer_id in densest_subedges:\n",
    "        if str(integer_id) not in _map:\n",
    "            return None\n",
    "        else:\n",
    "            _tmp.append(_map[str(integer_id)][1])\n",
    "            # print(_tmp)\n",
    "    return _tmp\n",
    "def retrieve_list_eventsCK(ck_dhg,mapN,hypfile):\n",
    "    # Only for clique\n",
    "    V_ck = set()\n",
    "    for e in ck_dhg:\n",
    "        for u in e:\n",
    "            V_ck.add(u)\n",
    "    densest_subedges = []\n",
    "    projected_hyp = []\n",
    "    with open(hypfile,'r') as hf:\n",
    "        count = 0\n",
    "        for line in hf:\n",
    "            l = line.strip().split(\",\")\n",
    "            s = set(l)\n",
    "            if s.issubset(V_ck):\n",
    "                densest_subedges.append(count)\n",
    "                projected_hyp.append(l)\n",
    "            count+=1\n",
    "    _map = load_dict(mapN)\n",
    "    _tmp = []\n",
    "    for integer_id in densest_subedges:\n",
    "        if str(integer_id) not in _map:\n",
    "            return None\n",
    "        else:\n",
    "            _tmp.append(_map[str(integer_id)][1])\n",
    "            # print(_tmp)\n",
    "    return projected_hyp, _tmp\n",
    "\n",
    "def get_volume_density(node_subset, dens_subhyp):\n",
    "    # Compute nbrs for all the nodes.\n",
    "    nbrs = {u: set() for u in node_subset}\n",
    "    for hyperedge in dens_subhyp:\n",
    "        absE = len(hyperedge)-1\n",
    "        for u in hyperedge:\n",
    "            for v in hyperedge:\n",
    "                if u!=v:\n",
    "                    nbrs[u].add(v)\n",
    "    print('average #participants: ',np.mean([len(hyperedge) for hyperedge in dens_subhyp]))\n",
    "    return sum([len(nbrs[u]) for u in node_subset])/len(node_subset)\n",
    "\n",
    "deg_dhg = load_hg_ascsv(deg_d)\n",
    "vol_dhg = load_hg_ascsv(vol_d)\n",
    "ck_dhg = load_hg_ascsv(ck_d)\n",
    "nodes_degD = set()\n",
    "for e in deg_dhg:\n",
    "    for u in e:\n",
    "        nodes_degD.add(u)\n",
    "\n",
    "nodes_volD = set()\n",
    "for e in vol_dhg:\n",
    "    for u in e:\n",
    "        nodes_volD.add(u)\n",
    "\n",
    "nodes_ckD = set()\n",
    "for e in ck_dhg:\n",
    "    for u in e:\n",
    "        nodes_ckD.add(u)\n",
    "        \n",
    "\n",
    "# print(nodes_degD)\n",
    "print('degree-Densest sub-hyp Events: ')\n",
    "coreprotein_names = retrieve_list_events(efile_degD,mapN=M)\n",
    "print(coreprotein_names)\n",
    "print('vol. density: ',get_volume_density(nodes_degD, deg_dhg))\n",
    "# sub_complexes_degD,deg_df = retrieve_stronglyinduced_emails(nodes_degD, hyperedges,df)\n",
    "# # print(deg_df['Subject'].head())\n",
    "print('volume-Densest sub-hyp Events: ')\n",
    "coreprotein_names = retrieve_list_events(efile_volD,mapN=M)\n",
    "print(coreprotein_names)\n",
    "print('volume-density: ',get_volume_density(nodes_volD, vol_dhg))\n",
    "# sub_complexes_volD,vol_df = retrieve_stronglyinduced_emails(nodes_volD, hyperedges,df)\n",
    "# # print(vol_df['Subject'].head())\n",
    "# # print(len(sub_complexes_volD))\n",
    "print('clique-Densest sub-hyp Events: ')\n",
    "projected_hyp, coreprotein_names_ck = retrieve_list_eventsCK(ck_dhg,mapN=M,hypfile = '../data/datasets/real/meetup.hyp') # Take union of list_of_edges in ck to find the nodes. Read hyp file\n",
    "print(coreprotein_names_ck)\n",
    "print('volume-density: ',get_volume_density(nodes_ckD, projected_hyp),' degree-density as clique: ',len(ck_dhg)/len(nodes_ckD))\n",
    "# sub_complexes_ckD,ck_df = retrieve_stronglyinduced_emails(nodes_ckD, hyperedges,df)\n",
    "# print('clique-Densest sub-protein complexes: ')\n",
    "# # print(ck_df['Subject'].head())\n",
    "# # print(len(sub_complexes_ckD))\n",
    "\n",
    "print('|V*_deg|=',len(nodes_degD),' |V*_vol|=',len(nodes_volD),' |V*_clique|=',len(nodes_ckD))\n",
    "print('|E*_deg|=',len(deg_dhg),' |E*_vol|=',len(vol_dhg),' |E*_clique|=',len(coreprotein_names_ck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b360353-fcc1-437e-adc9-7c1062f845a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b75b2-2a56-4dea-9596-1b1037199141",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Show our vol dens > Projected-back clique.\n",
    "2. Projection er jonno. Loss  korchi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44bbd588-a106-44a3-a232-2766328189c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = '../data/datasets/real/meetup_eidtoevent.map'\n",
    "# dataset = 'meetup'\n",
    "\n",
    "# deg_d = '../output/density/'+dataset+'_deg.csv'\n",
    "# vol_d = '../output/density/'+dataset+'_nbr.csv'\n",
    "# ck_d = '../output/density/'+dataset+'_clique.csv'\n",
    "\n",
    "# efile_degD = '../output/density/'+dataset+'_deg_e.csv'\n",
    "# efile_volD = '../output/density/'+dataset+'_nbr_e.csv'\n",
    "# efile_ckD = '../output/density/'+dataset+'_clique_e.csv'\n",
    "\n",
    "# def retrieve_list_events(densest_subeFile, mapN='../data/datasets/real/meetup_eidtoevent.map'):\n",
    "#     _map = load_dict(mapN)\n",
    "#     densest_subedges = []\n",
    "#     with open(densest_subeFile,'r') as f:\n",
    "#         for line in f:\n",
    "#             densest_subedges.append(int(line.strip()))\n",
    "#     # print(_map)\n",
    "#     _tmp = []\n",
    "#     for integer_id in densest_subedges:\n",
    "#         if str(integer_id) not in _map:\n",
    "#             return None\n",
    "#         else:\n",
    "#             _tmp.append(_map[str(integer_id)][1])\n",
    "#             # print(_tmp)\n",
    "#     return _tmp\n",
    "# def retrieve_list_eventsCK(ck_dhg,mapN,hypfile):\n",
    "#     # Only for clique\n",
    "#     V_ck = set()\n",
    "#     for e in ck_dhg:\n",
    "#         for u in e:\n",
    "#             V_ck.add(u)\n",
    "#     densest_subedges = []\n",
    "#     with open(hypfile,'r') as hf:\n",
    "#         count = 0\n",
    "#         for line in hf:\n",
    "#             l = line.strip().split(\",\")\n",
    "#             s = set(l)\n",
    "#             if s.issubset(V_ck):\n",
    "#                 densest_subedges.append(count)\n",
    "#             count+=1\n",
    "#     _map = load_dict(mapN)\n",
    "#     _tmp = []\n",
    "#     for integer_id in densest_subedges:\n",
    "#         if str(integer_id) not in _map:\n",
    "#             return None\n",
    "#         else:\n",
    "#             _tmp.append(_map[str(integer_id)][1])\n",
    "#             # print(_tmp)\n",
    "#     return list(V_ck), _tmp\n",
    "\n",
    "\n",
    "# deg_dhg = load_hg_ascsv(deg_d)\n",
    "# vol_dhg = load_hg_ascsv(vol_d)\n",
    "# ck_dhg = load_hg_ascsv(ck_d)\n",
    "# nodes_degD = set()\n",
    "# for e in deg_dhg:\n",
    "#     for u in e:\n",
    "#         nodes_degD.add(u)\n",
    "\n",
    "# nodes_volD = set()\n",
    "# for e in vol_dhg:\n",
    "#     for u in e:\n",
    "#         nodes_volD.add(u)\n",
    "\n",
    "# nodes_ckD = set()\n",
    "# for e in ck_dhg:\n",
    "#     for u in e:\n",
    "#         nodes_ckD.add(u)\n",
    "    \n",
    "# # print(nodes_degD)\n",
    "# print('degree-Densest sub-hyp Events: ')\n",
    "# coreprotein_names = retrieve_list_events(efile_degD,mapN=M)\n",
    "# print(coreprotein_names)\n",
    "# # sub_complexes_degD,deg_df = retrieve_stronglyinduced_emails(nodes_degD, hyperedges,df)\n",
    "# # # print(deg_df['Subject'].head())\n",
    "# print('volume-Densest sub-hyp Events: ')\n",
    "# coreprotein_names = retrieve_list_events(efile_volD,mapN=M)\n",
    "# print(coreprotein_names)\n",
    "# # sub_complexes_volD,vol_df = retrieve_stronglyinduced_emails(nodes_volD, hyperedges,df)\n",
    "# # # print(vol_df['Subject'].head())\n",
    "# # # print(len(sub_complexes_volD))\n",
    "# print('clique-Densest sub-hyp Events: ')\n",
    "# nodes_ckD, coreprotein_names_ck = retrieve_list_eventsCK(ck_dhg,mapN=M,hypfile = '../data/datasets/real/meetup.hyp') # Take union of list_of_edges in ck to find the nodes. Read hyp file\n",
    "# print(coreprotein_names_ck)\n",
    "# # sub_complexes_ckD,ck_df = retrieve_stronglyinduced_emails(nodes_ckD, hyperedges,df)\n",
    "# # print('clique-Densest sub-protein complexes: ')\n",
    "# # # print(ck_df['Subject'].head())\n",
    "# # # print(len(sub_complexes_ckD))\n",
    "\n",
    "# print(len(nodes_degD),' ',len(nodes_volD),' ',len(nodes_ckD))\n",
    "# print(len(deg_dhg),' ',len(vol_dhg),' ',len(coreprotein_names_ck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819f952-7507-4263-8624-f5afb54ed4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fdc1ca18-e3d6-4182-8362-74aade111333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dict(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94a6a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypergraph Class\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "class Hypergraph:\n",
    "    \"\"\" \n",
    "    Our own hypergraph representation class. \n",
    "    We store hyperedge list in compressed format using two things- 1) e_indices (a dict) 2) e_nodes (a list)\n",
    "    Although edge-centric queries (e.g. edge enumeration) are facilitated in this way, node-centric queries are not convenient.\n",
    "    To support node-centric queries, we also maintain incidence dictionary inc_dict (key = v_ids, values = incident edge ids)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _edgedict=None):\n",
    "        \n",
    "        self.e_indices = {}  # (position, position+edge_size) of edge e in e_nodes list\n",
    "        self.e_nodes = []  # flattened edge list\n",
    "        self.inc_dict = {}  # key: nodeid, value = ids of incident edges (set)\n",
    "        # degree pre-compute => degree_dict or len_incedge = {}\n",
    "        self.degree_dict = {}\n",
    "        self.init_nbrsize = {} # initial nbrhood sizes. can be precomputed.\n",
    "        self.init_nbr = {}\n",
    "        self.init_eids = {}\n",
    "        self.init_nodes = []\n",
    "        if _edgedict is None or len(_edgedict)==0:  # Returns an empty Hypergraph\n",
    "            return\n",
    "\n",
    "        self.i = 0\n",
    "        for e_id, e in _edgedict.items():\n",
    "            _len = len(e)\n",
    "            \n",
    "            self.e_indices[e_id] = (self.i, self.i + _len)\n",
    "            self.init_eids[e_id] = (self.i, self.i + _len)\n",
    "            for v in e:\n",
    "                self.e_nodes.append(v)\n",
    "                if v not in self.inc_dict:\n",
    "                    self.inc_dict[v] = set()  # create incident edge entry for v\n",
    "                    self.init_nodes.append(v)\n",
    "                self.inc_dict[v].add(e_id)  # incident edge update\n",
    "                self.degree_dict[v] = self.degree_dict.get(v, 0) + 1  # degree update\n",
    "                nbr_v = self.init_nbr.get(v, set()).union(e)\n",
    "                nbr_v.remove(v)\n",
    "                self.init_nbrsize[v] = len(nbr_v)  # neighbourhood length update\n",
    "                self.init_nbr[v] = nbr_v  # neighbourbood set update\n",
    "            self.i += _len\n",
    "\n",
    "        self.init_nodes = sorted(self.init_nodes)\n",
    "    def get_topk_degNodes(self,k):\n",
    "        \"\"\" Returns top-k highest degree nodes \"\"\"\n",
    "        import heapq\n",
    "        degree_loftup = list(self.degree_dict.items())\n",
    "        k_keys_sorted = heapq.nlargest(k, degree_loftup,lambda e: e[1])\n",
    "        return k_keys_sorted\n",
    "\n",
    "    def get_topk_nbrNodes(self,k):\n",
    "        \"\"\" Returns top-k highest degree nodes \"\"\"\n",
    "        import heapq\n",
    "        nbr_loftup = list(self.init_nbrsize.items())\n",
    "        k_keys_sorted = heapq.nlargest(k, nbr_loftup,lambda e: e[1])\n",
    "        return k_keys_sorted\n",
    "\n",
    "    def get_topk_hyperedges(self,k):\n",
    "        \"\"\" Returns top-k largest hyperedges \"\"\"\n",
    "        import heapq \n",
    "\n",
    "        _tmp = []\n",
    "        for eid in self.e_indices:\n",
    "            _tmp.append((eid,self.e_indices[eid][1] - self.e_indices[eid][0]))\n",
    "        return heapq.nlargest(k, _tmp,lambda e: e[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.e_indices)\n",
    "def get_topk_listoftuples(k,listoftups):\n",
    "    import heapq\n",
    "    k_keys_sorted = heapq.nlargest(k, listoftups,lambda tup: tup[1])\n",
    "    return k_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd1dee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k degree nodes in the  vol-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('jeff.skilling@enron.com', 'Jeffery Skilling', 'CEO'), ('mark.frevert@enron.com', 'Mark A. Frevert', 'Chairman and CEO(Enron WholeSale Services)'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)')]\n",
      "top-k nbr nodes in the  vol-densest sub-hyp: \n",
      "[('greg.martin@enron.com', 'Gregory Martin', 'Analyst'), ('dustin.collins@enron.com', 'Dustin Collins', 'Associate(Enron Global Commodities)'), ('andrea.richards@enron.com', None, None), ('sladana-anna.kulic@enron.com', None, None), ('maureen.mcvicker@enron.com', 'Maureen McVicker', 'Assistant(Steven Kean)')]\n",
      "----------\n",
      "top-k degree nodes in the  degree-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)'), ('jeffrey.mcmahon@enron.com', 'Jeffrey McMahon', 'Chief Financial Officer'), ('mark.frevert@enron.com', 'Mark A. Frevert', 'Chairman and CEO(Enron WholeSale Services)')]\n",
      "top-k nbr nodes in the  degree-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)'), ('jeffrey.mcmahon@enron.com', 'Jeffrey McMahon', 'Chief Financial Officer'), ('john.sherriff@enron.com', ' John Sherriff', 'President and CEO(Enron Europe)')]\n",
      "---------------\n",
      "top-k degree nodes in the clique-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('jeff.skilling@enron.com', 'Jeffery Skilling', 'CEO'), ('mark.frevert@enron.com', 'Mark A. Frevert', 'Chairman and CEO(Enron WholeSale Services)'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)')]\n",
      "top-k nbr nodes in the  clique-densest sub-hyp: \n",
      "[('greg.martin@enron.com', 'Gregory Martin', 'Analyst'), ('dustin.collins@enron.com', 'Dustin Collins', 'Associate(Enron Global Commodities)'), ('andrea.richards@enron.com', None, None), ('sladana-anna.kulic@enron.com', None, None), ('maureen.mcvicker@enron.com', 'Maureen McVicker', 'Assistant(Steven Kean)')]\n"
     ]
    }
   ],
   "source": [
    "def retrieve_list_emails(l):\n",
    "    \n",
    "vol_dhg = Hypergraph({i: hyperedges[i] for i in sub_complexes_volD})\n",
    "# print(len(vol_dhg.init_nodes),' ',len(vol_dhg))\n",
    "voldhg_topk = vol_dhg.get_topk_nbrNodes(topk)\n",
    "degdhg_topk = vol_dhg.get_topk_degNodes(topk)\n",
    "\n",
    "topk_nbr_emails= retrieve_list_emails([i[0] for i in voldhg_topk])\n",
    "topk_deg_emails = retrieve_list_emails([i[0] for i in degdhg_topk])\n",
    "# print(topk_nbr_emails)\n",
    "\n",
    "\n",
    "print('top-k degree nodes in the  vol-densest sub-hyp: ')\n",
    "# print([(i,query_position(i)) for i in topk_deg_emails])\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_deg_emails])\n",
    "# print([vol_dhg.degree_dict[i[0]] for i in degdhg_topk])\n",
    "\n",
    "print('top-k nbr nodes in the  vol-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_nbr_emails])\n",
    "# print([vol_dhg.init_nbrsize[i[0]] for i in voldhg_topk])\n",
    "print('----------')\n",
    "deg_dhg = Hypergraph({i: hyperedges[i] for i in sub_complexes_degD})\n",
    "# print(len(deg_dhg.init_nodes),' ',len(deg_dhg))\n",
    "# print(len(deg_dhg))\n",
    "degdhg_topk =deg_dhg.get_topk_degNodes(topk)\n",
    "voldhg_topk = deg_dhg.get_topk_nbrNodes(topk)\n",
    "topk_nbr_emails= retrieve_list_emails([i[0] for i in voldhg_topk])\n",
    "topk_deg_emails = retrieve_list_emails([i[0] for i in degdhg_topk])\n",
    "print('top-k degree nodes in the  degree-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_deg_emails])\n",
    "# print([deg_dhg.degree_dict[i[0]] for i in degdhg_topk])\n",
    "print('top-k nbr nodes in the  degree-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_nbr_emails])\n",
    "# print([deg_dhg.init_nbrsize[i[0]] for i in voldhg_topk])\n",
    "# print(topk_deg_emails)\n",
    "# print([deg_dhg.degree_dict[u] for u in deg_dhg.get_topk_degNodes(topk)])\n",
    "print('---------------')\n",
    "ck_dhg = Hypergraph({i: hyperedges[i] for i in sub_complexes_ckD})\n",
    "degdhg_topk =ck_dhg.get_topk_degNodes(topk)\n",
    "voldhg_topk = ck_dhg.get_topk_nbrNodes(topk)\n",
    "topk_nbr_emails= retrieve_list_emails([i[0] for i in voldhg_topk])\n",
    "topk_deg_emails = retrieve_list_emails([i[0] for i in degdhg_topk])\n",
    "print('top-k degree nodes in the clique-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_deg_emails])\n",
    "print('top-k nbr nodes in the  clique-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_nbr_emails])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "48019a02b954b0950cef4fb08df99e126fc693d05fb154c23590c295a357a523"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
