{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9062623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topk = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b492b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/datasets/kenneth_lay/enron_roles.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31404\\3329916307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mroles_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/datasets/kenneth_lay/enron_roles.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mroles_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mroles_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroles_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mroles_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'emailid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroles_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/datasets/kenneth_lay/enron_roles.txt'"
     ]
    }
   ],
   "source": [
    "roles_path = '../data/datasets/kenneth_lay/enron_roles.txt'\n",
    "roles_dict = {}\n",
    "roles_df = pd.read_csv(roles_path,sep=\",\")\n",
    "roles_df.columns = ['emailid','name','position']\n",
    "print(roles_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2ef3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgname = '../data/datasets/kenneth_lay/klay_s.hyp'\n",
    "hyperedges = []\n",
    "vertex_set = set()\n",
    "with open(hgname,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        edge = line.split(',')\n",
    "        edge = [e.strip() for e in edge]\n",
    "        hyperedges.append(edge)\n",
    "        for v in edge:\n",
    "            vertex_set.add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68492ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                     Message-ID  \\\n",
      "0          0  <19937858.1075858631491.JavaMail.evans@thyme>   \n",
      "1          1  <20682410.1075859177338.JavaMail.evans@thyme>   \n",
      "2          2  <25469111.1075840337794.JavaMail.evans@thyme>   \n",
      "3          4  <31732110.1075849814292.JavaMail.evans@thyme>   \n",
      "4          5   <5072789.1075855783561.JavaMail.evans@thyme>   \n",
      "\n",
      "                        Date  \\\n",
      "0  2001-10-18 11:52:23-07:00   \n",
      "1  2001-07-14 12:33:04-07:00   \n",
      "2  2001-08-23 11:24:25-07:00   \n",
      "3  2001-01-03 10:58:00-08:00   \n",
      "4  2000-08-25 08:26:00-07:00   \n",
      "\n",
      "                                             Subject  \\\n",
      "0  Quarterly Managing Director Meeting - Monday, ...   \n",
      "1  FW: Iris Mack's involvement in the Enron Team ...   \n",
      "2                          Associate/Analyst Program   \n",
      "3                             1/2/01 Preliminary DPR   \n",
      "4                                      DPR - 8/24/00   \n",
      "\n",
      "                                            X-Folder    X-Origin  \\\n",
      "0  \\PALLEN (Non-Privileged)\\Allen, Phillip K.\\Del...     Allen-P   \n",
      "1   \\Harry_Arora_Jan2002\\Arora, Harry\\Inbox\\Receipts     Arora-H   \n",
      "2   \\ExMerge - Baughman Jr., Don\\Enron Power\\24 Hour  BAUGHMAN-D   \n",
      "3    \\Sally_Beck_Nov2001\\Notes Folders\\All documents      BECK-S   \n",
      "4    \\Sally_Beck_Dec2000\\Notes Folders\\All documents      Beck-S   \n",
      "\n",
      "                    X-FileName  \\\n",
      "0  PALLEN (Non-Privileged).pst   \n",
      "1  harora (Non-Privileged).pst   \n",
      "2     don baughman 6-25-02.PST   \n",
      "3                    sbeck.nsf   \n",
      "4                    sbeck.nsf   \n",
      "\n",
      "                                             content        user  \\\n",
      "0  Please plan to attend the Quarterly Managing D...     allen-p   \n",
      "1  \\nDear Dr. Lay,\\n\\n\\tMy name is Iris Mack.  My...     arora-h   \n",
      "2   \\n\\nTo:\\tAssociate/Analyst Program Worldwide\\...  baughman-d   \n",
      "3     \\nA preliminary Daily Position Report has b...      beck-s   \n",
      "4  The DPR for today is final and posted to the E...      beck-s   \n",
      "\n",
      "                                        participants  ... Unnamed: 33  \\\n",
      "0  jim.prentice@enron.com,mark.frevert@enron.com,...  ...         NaN   \n",
      "1          iris.mack@enron.com,kenneth.lay@enron.com  ...         NaN   \n",
      "2  lindsay.culotta@enron.com,mark.frevert@enron.c...  ...         NaN   \n",
      "3  mark.haedicke@enron.com,jeffrey.shankman@enron...  ...         NaN   \n",
      "4  mark.haedicke@enron.com,jeffrey.shankman@enron...  ...         NaN   \n",
      "\n",
      "  Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 40 Unnamed: 41 Unnamed: 42  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "1190\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/datasets/kenneth_lay/klay_s.csv',sep=',',engine = 'python')\n",
    "print(df.head())\n",
    "print(len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987cee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vertex:  4718  #edge:  1190\n"
     ]
    }
   ],
   "source": [
    "print('#vertex: ',len(vertex_set),' #edge: ',len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe28246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(a, fname = 'tmp.pickle'):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_dict(fname = 'tmp.pickle'):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "        return b\n",
    "    \n",
    "def retrieve_email(integer_id,mapN='../data/datasets/kenneth_lay/klay_idtoemail.map'):\n",
    "    _map = load_dict(mapN)\n",
    "    if str(integer_id) not in _map:\n",
    "        return None\n",
    "    return _map[str(integer_id)]\n",
    "\n",
    "def retrieve_list_emails(lst_integer_id,mapN='../data/datasets/kenneth_lay/klay_idtoemail.map'):\n",
    "    _map = load_dict(mapN)\n",
    "    # print(_map)\n",
    "    _tmp = []\n",
    "    for integer_id in lst_integer_id:\n",
    "        if str(integer_id) not in _map:\n",
    "            return None\n",
    "        else:\n",
    "            _tmp.append(_map[str(integer_id)])\n",
    "    return _tmp\n",
    "\n",
    "def retrieve_stronglyinduced_emails(lst_integer_id, edgeList, df):\n",
    "    subhyp = []\n",
    "    for i,value in enumerate(edgeList):\n",
    "        flag = True\n",
    "        for u in value:\n",
    "            if u not in lst_integer_id:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            subhyp.append(i)\n",
    "        \n",
    "    return subhyp, df.filter(items = subhyp, axis=0)\n",
    "\n",
    "# def retrieve_weeklyinduced_complexes(lst_integer_id,human_hg):\n",
    "#     protein_complex_ids = []\n",
    "#     for key,value in human_hg.items():\n",
    "#         flag = True\n",
    "#         for u in value:\n",
    "#             if u in lst_integer_id:\n",
    "#                 protein_complex_ids.append(key)\n",
    "#                 break \n",
    "#     return protein_complex_ids\n",
    "def load_hg_ascsv(name):\n",
    "    _list = []\n",
    "    with open(name,'r') as f:\n",
    "        for line in f:\n",
    "            e = [i for i in line.strip().split(',')]\n",
    "            _list.append(e)\n",
    "    return _list\n",
    "\n",
    "def query_position(email):\n",
    "    em_id = email.split('@')[0]\n",
    "    _tmp = roles_df[roles_df.emailid == em_id]\n",
    "    if len(_tmp) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return roles_df[roles_df.emailid == em_id].position.values[0]\n",
    "def query_name(email):\n",
    "    em_id = email.split('@')[0]\n",
    "    _tmp = roles_df[roles_df.emailid == em_id]\n",
    "    if len(_tmp) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return roles_df[roles_df.emailid == em_id].name.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a9972",
   "metadata": {},
   "source": [
    "Densest Subgraph Analysis\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4340cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166   1949\n",
      "202   122\n",
      "degree-Densest sub-protein complexes: \n",
      "6      EnronOnline Executive Summary for April 20, 2001\n",
      "7     EnronOnline Executive Summary for December 12,...\n",
      "8        EnronOnline Executive Summary for May 31, 2001\n",
      "12                     NEW POLL FOR MC EXTENDED MEETING\n",
      "13                 New Poll for the MC Extended Meeting\n",
      "Name: Subject, dtype: object\n",
      "volume-Densest sub-protein complexes: \n",
      "2                        Associate/Analyst Program\n",
      "19                     Center for Houston's Future\n",
      "24    Project Southwood Meeting  - Location Change\n",
      "95                                          UC/CSU\n",
      "97                      III Summit of the Americas\n",
      "Name: Subject, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = 'klay'\n",
    "deg_d = '../output/density/'+dataset+'_deg.csv'\n",
    "vol_d = '../output/density/'+dataset+'_nbr.csv'\n",
    "deg_dhg = load_hg_ascsv(deg_d)\n",
    "vol_dhg = load_hg_ascsv(vol_d)\n",
    "nodes_degD = set()\n",
    "for e in deg_dhg:\n",
    "    for u in e:\n",
    "        nodes_degD.add(u)\n",
    "\n",
    "nodes_volD = set()\n",
    "for e in vol_dhg:\n",
    "    for u in e:\n",
    "        nodes_volD.add(u)\n",
    "\n",
    "print(len(nodes_degD),' ',len(nodes_volD))\n",
    "print(len(deg_dhg),' ',len(vol_dhg))\n",
    "# print(nodes_degD)\n",
    "coreprotein_names = retrieve_list_emails(nodes_degD,mapN='../data/datasets/kenneth_lay/'+dataset+'_idtoemail.map')\n",
    "# print(coreprotein_names)\n",
    "sub_complexes_degD,deg_df = retrieve_stronglyinduced_emails(nodes_degD, hyperedges,df)\n",
    "print('degree-Densest sub-protein complexes: ')\n",
    "print(deg_df['Subject'].head())\n",
    "coreprotein_names = retrieve_list_emails(nodes_volD,mapN='../data/datasets/kenneth_lay/'+dataset+'_idtoemail.map')\n",
    "# print(coreprotein_names)\n",
    "sub_complexes_volD,vol_df = retrieve_stronglyinduced_emails(nodes_volD, hyperedges,df)\n",
    "print('volume-Densest sub-protein complexes: ')\n",
    "print(vol_df['Subject'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a6a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypergraph Class\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "class Hypergraph:\n",
    "    \"\"\" \n",
    "    Our own hypergraph representation class. \n",
    "    We store hyperedge list in compressed format using two things- 1) e_indices (a dict) 2) e_nodes (a list)\n",
    "    Although edge-centric queries (e.g. edge enumeration) are facilitated in this way, node-centric queries are not convenient.\n",
    "    To support node-centric queries, we also maintain incidence dictionary inc_dict (key = v_ids, values = incident edge ids)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _edgedict=None):\n",
    "        \n",
    "        self.e_indices = {}  # (position, position+edge_size) of edge e in e_nodes list\n",
    "        self.e_nodes = []  # flattened edge list\n",
    "        self.inc_dict = {}  # key: nodeid, value = ids of incident edges (set)\n",
    "        # degree pre-compute => degree_dict or len_incedge = {}\n",
    "        self.degree_dict = {}\n",
    "        self.init_nbrsize = {} # initial nbrhood sizes. can be precomputed.\n",
    "        self.init_nbr = {}\n",
    "        self.init_eids = {}\n",
    "        self.init_nodes = []\n",
    "        if _edgedict is None or len(_edgedict)==0:  # Returns an empty Hypergraph\n",
    "            return\n",
    "\n",
    "        self.i = 0\n",
    "        for e_id, e in _edgedict.items():\n",
    "            _len = len(e)\n",
    "            \n",
    "            self.e_indices[e_id] = (self.i, self.i + _len)\n",
    "            self.init_eids[e_id] = (self.i, self.i + _len)\n",
    "            for v in e:\n",
    "                self.e_nodes.append(v)\n",
    "                if v not in self.inc_dict:\n",
    "                    self.inc_dict[v] = set()  # create incident edge entry for v\n",
    "                    self.init_nodes.append(v)\n",
    "                self.inc_dict[v].add(e_id)  # incident edge update\n",
    "                self.degree_dict[v] = self.degree_dict.get(v, 0) + 1  # degree update\n",
    "                nbr_v = self.init_nbr.get(v, set()).union(e)\n",
    "                nbr_v.remove(v)\n",
    "                self.init_nbrsize[v] = len(nbr_v)  # neighbourhood length update\n",
    "                self.init_nbr[v] = nbr_v  # neighbourbood set update\n",
    "            self.i += _len\n",
    "\n",
    "        self.init_nodes = sorted(self.init_nodes)\n",
    "    def get_topk_degNodes(self,k):\n",
    "        \"\"\" Returns top-k highest degree nodes \"\"\"\n",
    "        import heapq\n",
    "        degree_loftup = list(self.degree_dict.items())\n",
    "        k_keys_sorted = heapq.nlargest(k, degree_loftup,lambda e: e[1])\n",
    "        return k_keys_sorted\n",
    "\n",
    "    def get_topk_nbrNodes(self,k):\n",
    "        \"\"\" Returns top-k highest degree nodes \"\"\"\n",
    "        import heapq\n",
    "        nbr_loftup = list(self.init_nbrsize.items())\n",
    "        k_keys_sorted = heapq.nlargest(k, nbr_loftup,lambda e: e[1])\n",
    "        return k_keys_sorted\n",
    "\n",
    "    def get_topk_hyperedges(self,k):\n",
    "        \"\"\" Returns top-k largest hyperedges \"\"\"\n",
    "        import heapq \n",
    "\n",
    "        _tmp = []\n",
    "        for eid in self.e_indices:\n",
    "            _tmp.append((eid,self.e_indices[eid][1] - self.e_indices[eid][0]))\n",
    "        return heapq.nlargest(k, _tmp,lambda e: e[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.e_indices)\n",
    "def get_topk_listoftuples(k,listoftups):\n",
    "    import heapq\n",
    "    k_keys_sorted = heapq.nlargest(k, listoftups,lambda tup: tup[1])\n",
    "    return k_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1dee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k degree nodes in the  vol-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('jeff.skilling@enron.com', 'Jeffery Skilling', 'CEO'), ('mark.frevert@enron.com', 'Mark A. Frevert', 'Chairman and CEO(Enron WholeSale Services)'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)')]\n",
      "top-k nbr nodes in the  vol-densest sub-hyp: \n",
      "[('greg.martin@enron.com', 'Gregory Martin', 'Analyst'), ('dustin.collins@enron.com', 'Dustin Collins', 'Associate(Enron Global Commodities)'), ('andrea.richards@enron.com', None, None), ('sladana-anna.kulic@enron.com', None, None), ('maureen.mcvicker@enron.com', 'Maureen McVicker', 'Assistant(Steven Kean)')]\n",
      "----------\n",
      "top-k degree nodes in the  degree-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)'), ('jeffrey.mcmahon@enron.com', 'Jeffrey McMahon', 'Chief Financial Officer'), ('mark.frevert@enron.com', 'Mark A. Frevert', 'Chairman and CEO(Enron WholeSale Services)')]\n",
      "top-k nbr nodes in the  vol-densest sub-hyp: \n",
      "[('kenneth.lay@enron.com', 'Kenneth Lay', 'CEO'), ('greg.whalley@enron.com', 'Greg Whalley', 'President'), ('mark.koenig@enron.com', 'Mark Koenig', 'Head(Investor Relations)'), ('jeffrey.mcmahon@enron.com', 'Jeffrey McMahon', 'Chief Financial Officer'), ('john.sherriff@enron.com', ' John Sherriff', 'President and CEO(Enron Europe)')]\n"
     ]
    }
   ],
   "source": [
    "vol_dhg = Hypergraph({i: hyperedges[i] for i in sub_complexes_volD})\n",
    "# print(len(vol_dhg.init_nodes),' ',len(vol_dhg))\n",
    "voldhg_topk = vol_dhg.get_topk_nbrNodes(topk)\n",
    "degdhg_topk = vol_dhg.get_topk_degNodes(topk)\n",
    "topk_nbr_emails= retrieve_list_emails([i[0] for i in voldhg_topk])\n",
    "topk_deg_emails = retrieve_list_emails([i[0] for i in degdhg_topk])\n",
    "# print(topk_nbr_emails)\n",
    "\n",
    "\n",
    "print('top-k degree nodes in the  vol-densest sub-hyp: ')\n",
    "# print([(i,query_position(i)) for i in topk_deg_emails])\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_deg_emails])\n",
    "# print([vol_dhg.degree_dict[i[0]] for i in degdhg_topk])\n",
    "\n",
    "print('top-k nbr nodes in the  vol-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_nbr_emails])\n",
    "# print([vol_dhg.init_nbrsize[i[0]] for i in voldhg_topk])\n",
    "print('----------')\n",
    "deg_dhg = Hypergraph({i: hyperedges[i] for i in sub_complexes_degD})\n",
    "# print(len(deg_dhg.init_nodes),' ',len(deg_dhg))\n",
    "# print(len(deg_dhg))\n",
    "degdhg_topk =deg_dhg.get_topk_degNodes(topk)\n",
    "voldhg_topk = deg_dhg.get_topk_nbrNodes(topk)\n",
    "topk_nbr_emails= retrieve_list_emails([i[0] for i in voldhg_topk])\n",
    "topk_deg_emails = retrieve_list_emails([i[0] for i in degdhg_topk])\n",
    "print('top-k degree nodes in the  degree-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_deg_emails])\n",
    "# print([deg_dhg.degree_dict[i[0]] for i in degdhg_topk])\n",
    "print('top-k nbr nodes in the  vol-densest sub-hyp: ')\n",
    "print([(i,query_name(i),query_position(i)) for i in topk_nbr_emails])\n",
    "# print([deg_dhg.init_nbrsize[i[0]] for i in voldhg_topk])\n",
    "# print(topk_deg_emails)\n",
    "# print([deg_dhg.degree_dict[u] for u in deg_dhg.get_topk_degNodes(topk)])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63df9e395d890b85a8eeedf0e486c3a26dbe81c6ae922fd0c3f97bf5869b1ef1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
